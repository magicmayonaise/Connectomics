"""Utilities for tracing multi-hop synaptic adjacency layers.

The helpers in this module build upon the FAFB materialization APIs exposed by
``caveclient`` (or compatible clients) to walk synaptic connectivity graphs.

The new ``trace_next_layer`` helper extends a previously computed
``N1 → N2`` adjacency to a second hop ``N2 → N3`` while enforcing node
deduplication across layers and exporting convenient tabular summaries.
"""

from __future__ import annotations

import logging
from dataclasses import dataclass
from pathlib import Path
from typing import TYPE_CHECKING, Any, Callable, Dict, Iterable, Mapping, MutableMapping, Optional, Sequence, Union

try:  # pragma: no cover - optional dependency imported lazily when available
    import pandas as pd
except ModuleNotFoundError:  # pragma: no cover - handled gracefully at runtime
    pd = None  # type: ignore[assignment]

if TYPE_CHECKING:  # pragma: no cover - used only for type checkers
    import pandas as _pd

logger = logging.getLogger(__name__)


# ``synapses_nt_v1`` is the public FAFB synapse table.  The name is configurable so
# that downstream projects can re-use the helper with bespoke tables.
DEFAULT_SYNAPSE_TABLE = "synapses_nt_v1"
DEFAULT_MIN_SYNAPSES = 10
DEFAULT_OUTPUT_DIR = Path("out")

ADJACENCY_FILENAME = "N2_to_N3_adjacency.parquet"
N2_NODES_FILENAME = "N2_nodes.csv"
N3_NODES_FILENAME = "N3_nodes.csv"


try:  # pragma: no cover - optional dependency used when available
    from connectomics_gui.annotations import fetch_cell_types as _default_annotation_fetcher
except Exception:  # pragma: no cover - ``connectomics_gui`` is optional for this module
    _default_annotation_fetcher = None


AnnotationResult = Mapping[int, Optional[Mapping[str, Any]]]
AnnotationFetcher = Callable[..., AnnotationResult]


@dataclass(frozen=True)
class TraceResult:
    """Container for the adjacency and node tables generated by the trace."""

    adjacency: "_pd.DataFrame"
    n2_nodes: "_pd.DataFrame"
    n3_nodes: "_pd.DataFrame"


def trace_next_layer(
    client: Any,
    n1_to_n2: Union[pd.DataFrame, Sequence[Mapping[str, Any]], str, Path],
    threshold: int = DEFAULT_MIN_SYNAPSES,
    *,
    materialization: Optional[int] = None,
    at_ts: Optional[str] = None,
    synapse_table: str = DEFAULT_SYNAPSE_TABLE,
    output_dir: Union[str, Path] = DEFAULT_OUTPUT_DIR,
    annotation_fetcher: Optional[AnnotationFetcher] = None,
    annotation_kwargs: Optional[Mapping[str, Any]] = None,
    select_columns: Optional[Sequence[str]] = None,
) -> TraceResult:
    """Trace an ``N2 → N3`` adjacency layer given a previously computed ``N1 → N2`` layer.

    Parameters
    ----------
    client:
        CAVEclient (or compatible) instance that exposes ``materialize.query_table``.
    n1_to_n2:
        Representation of the previously computed adjacency.  Accepts a
        :class:`pandas.DataFrame`, an iterable of mappings, or a path to a CSV or
        Parquet file on disk.
    threshold:
        Minimum synapse count required for the ``N2 → N3`` adjacency.  Defaults to
        ``10``.
    materialization / at_ts:
        Exactly one of ``materialization`` (materialization version) or ``at_ts``
        (timestamp) may be provided.  They are forwarded to the materialization
        client when querying synapses.
    synapse_table:
        Name of the synapse table to query.  Defaults to ``synapses_nt_v1``.
    output_dir:
        Directory where tabular artefacts are written.  The directory is created
        if necessary.
    annotation_fetcher:
        Optional callable used to fetch annotations for nodes in the ``N2`` and
        ``N3`` sets.  When ``None`` the helper tries to import
        :func:`connectomics_gui.annotations.fetch_cell_types`.
    annotation_kwargs:
        Extra keyword arguments forwarded to ``annotation_fetcher``.
    select_columns:
        Optional sequence of column names to request from the synapse table.  The
        defaults request at least ``pre_pt_root_id`` and ``post_pt_root_id``.

    Returns
    -------
    TraceResult
        Dataclass containing the ``N2 → N3`` adjacency along with CSV exports of
        the ``N2`` and ``N3`` node sets (including merged annotations).
    """

    if pd is None:
        raise ImportError("pandas is required to use trace_next_layer")

    if materialization is not None and at_ts is not None:
        raise ValueError("Specify only one of materialization or at_ts, not both.")

    if threshold <= 0:
        raise ValueError("threshold must be a positive integer")

    adjacency_df = _ensure_dataframe(n1_to_n2)
    required_columns = {"pre_pt_root_id", "post_pt_root_id"}
    missing = required_columns - set(adjacency_df.columns)
    if missing:
        raise ValueError(
            "Missing required columns from previous adjacency: " + ", ".join(sorted(missing))
        )

    adjacency_df = adjacency_df.dropna(subset=["pre_pt_root_id", "post_pt_root_id"])
    adjacency_df = adjacency_df.drop_duplicates(subset=["pre_pt_root_id", "post_pt_root_id"])

    n1_nodes = _normalize_root_ids(adjacency_df["pre_pt_root_id"].tolist())
    n2_candidates = _normalize_root_ids(adjacency_df["post_pt_root_id"].tolist())

    # Deduplicate across layers by excluding N1 nodes from the N2 set.
    n2_nodes = sorted(node for node in n2_candidates if node not in n1_nodes)
    logger.debug("Identified %d unique N2 nodes", len(n2_nodes))

    output_directory = Path(output_dir)
    output_directory.mkdir(parents=True, exist_ok=True)

    if not n2_nodes:
        logger.info("No N2 nodes identified from previous adjacency; returning empty results.")
        empty_adjacency = _empty_adjacency_frame()
        n2_df = _nodes_dataframe([], {})
        n3_df = _nodes_dataframe([], {})
        _write_outputs(empty_adjacency, n2_df, n3_df, output_directory)
        return TraceResult(adjacency=empty_adjacency, n2_nodes=n2_df, n3_nodes=n3_df)

    synapse_records = _fetch_synapse_records(
        client,
        n2_nodes,
        synapse_table=synapse_table,
        materialization=materialization,
        at_ts=at_ts,
        select_columns=select_columns,
    )

    n2_to_n3 = _aggregate_synapses(synapse_records)

    if not n2_to_n3.empty:
        n2_to_n3 = n2_to_n3[n2_to_n3["synapse_count"] >= int(threshold)]
        n2_to_n3 = n2_to_n3.sort_values(
            ["pre_pt_root_id", "synapse_count", "post_pt_root_id"],
            ascending=[True, False, True],
            kind="mergesort",
        ).reset_index(drop=True)
    else:
        n2_to_n3 = _empty_adjacency_frame()

    n3_candidates = _normalize_root_ids(n2_to_n3["post_pt_root_id"].tolist())
    seen_nodes = set(n1_nodes) | set(n2_nodes)
    n3_nodes = sorted(node for node in n3_candidates if node not in seen_nodes)
    logger.debug("Identified %d unique N3 nodes", len(n3_nodes))

    annotations = _collect_annotations(
        n2_nodes,
        n3_nodes,
        annotation_fetcher=annotation_fetcher,
        annotation_kwargs=annotation_kwargs,
        materialization=materialization,
    )

    n2_df = _nodes_dataframe(n2_nodes, annotations)
    n3_df = _nodes_dataframe(n3_nodes, annotations)

    _write_outputs(n2_to_n3, n2_df, n3_df, output_directory)

    return TraceResult(adjacency=n2_to_n3, n2_nodes=n2_df, n3_nodes=n3_df)


def _ensure_dataframe(
    payload: Union[pd.DataFrame, Sequence[Mapping[str, Any]], str, Path]
) -> pd.DataFrame:
    if isinstance(payload, pd.DataFrame):
        return payload.copy()

    if isinstance(payload, (str, Path)):
        path = Path(payload)
        if not path.exists():
            raise FileNotFoundError(f"Adjacency source {path!s} does not exist")
        if path.suffix.lower() in {".parquet", ".pq"}:
            return pd.read_parquet(path)
        if path.suffix.lower() in {".csv", ".tsv"}:
            sep = "," if path.suffix.lower() == ".csv" else "\t"
            return pd.read_csv(path, sep=sep)
        raise ValueError(f"Unsupported adjacency file extension: {path.suffix}")

    if isinstance(payload, Sequence):
        try:
            return pd.DataFrame.from_records(list(payload))
        except Exception as exc:  # pragma: no cover - defensive programming
            raise TypeError("Could not convert sequence to DataFrame") from exc

    raise TypeError(
        "Unsupported adjacency input. Provide a DataFrame, a sequence of mappings, or a path to a table."
    )


def _fetch_synapse_records(
    client: Any,
    pre_root_ids: Sequence[int],
    *,
    synapse_table: str,
    materialization: Optional[int],
    at_ts: Optional[str],
    select_columns: Optional[Sequence[str]] = None,
) -> Sequence[Mapping[str, Any]]:
    materialize = getattr(client, "materialize", None)
    if materialize is None:
        raise AttributeError("Client does not expose a materialize attribute")

    query_fn = getattr(materialize, "query_table", None)
    if query_fn is None:
        raise AttributeError("Materialize interface does not provide query_table()")

    filters = {"pre_pt_root_id": sorted(int(root_id) for root_id in set(pre_root_ids))}
    query_kwargs: Dict[str, Any] = {
        "filter_in_dict": filters,
    }

    if select_columns is None:
        query_kwargs["select_columns"] = ["pre_pt_root_id", "post_pt_root_id", "synapse_count"]
    else:
        query_kwargs["select_columns"] = list(select_columns)

    if materialization is not None:
        query_kwargs["materialization_version"] = materialization
    if at_ts is not None:
        query_kwargs["timestamp"] = at_ts

    logger.debug(
        "Querying %s for %d presynaptic ids (materialization=%s, at_ts=%s)",
        synapse_table,
        len(pre_root_ids),
        materialization,
        at_ts,
    )

    result = query_fn(synapse_table, **query_kwargs)  # type: ignore[misc]
    return _normalize_records(result)


def _aggregate_synapses(records: Sequence[Mapping[str, Any]]) -> pd.DataFrame:
    if not records:
        return _empty_adjacency_frame()

    frame = pd.DataFrame.from_records(records)
    if frame.empty:
        return _empty_adjacency_frame()

    if "pre_pt_root_id" not in frame.columns or "post_pt_root_id" not in frame.columns:
        raise ValueError("Synapse records must include pre_pt_root_id and post_pt_root_id columns")

    frame = frame.copy()
    frame["pre_pt_root_id"] = pd.to_numeric(frame["pre_pt_root_id"], errors="coerce")
    frame["post_pt_root_id"] = pd.to_numeric(frame["post_pt_root_id"], errors="coerce")
    frame = frame.dropna(subset=["pre_pt_root_id", "post_pt_root_id"])
    frame["pre_pt_root_id"] = frame["pre_pt_root_id"].astype("int64")
    frame["post_pt_root_id"] = frame["post_pt_root_id"].astype("int64")

    if "synapse_count" in frame.columns:
        frame["synapse_count"] = pd.to_numeric(frame["synapse_count"], errors="coerce").fillna(0)
        frame["synapse_count"] = frame["synapse_count"].astype("int64")
        aggregated = (
            frame.groupby(["pre_pt_root_id", "post_pt_root_id"], as_index=False)["synapse_count"].sum()
        )
    else:
        aggregated = (
            frame.groupby(["pre_pt_root_id", "post_pt_root_id"], as_index=False)
            .size()
            .rename(columns={"size": "synapse_count"})
        )

    aggregated = aggregated.sort_values(
        ["pre_pt_root_id", "post_pt_root_id"],
        ascending=[True, True],
        kind="mergesort",
    ).reset_index(drop=True)

    return aggregated


def _normalize_records(result: Any) -> Sequence[Mapping[str, Any]]:
    if result is None:
        return []

    if isinstance(result, Mapping):
        return [result]

    if isinstance(result, Sequence) and not isinstance(result, (str, bytes, bytearray)):
        normalized: list[Mapping[str, Any]] = []
        for item in result:
            if isinstance(item, Mapping):
                normalized.append(item)
            elif hasattr(item, "_asdict"):
                normalized.append(item._asdict())
            else:
                normalized.append(dict(item))
        return normalized

    to_dict = getattr(result, "to_dict", None)
    if callable(to_dict):
        try:
            converted = to_dict("records")
        except TypeError:
            converted = to_dict()
        if isinstance(converted, Sequence):
            return [item if isinstance(item, Mapping) else dict(item) for item in converted]
        if isinstance(converted, Mapping):
            keys = list(converted.keys())
            length = len(converted[keys[0]]) if keys else 0
            rows: list[Dict[str, Any]] = []
            for index in range(length):
                rows.append({key: converted[key][index] for key in keys})
            return rows

    raise TypeError(f"Unsupported synapse query result type: {type(result)!r}")


def _normalize_root_ids(root_ids: Iterable[Any]) -> list[int]:
    seen: set[int] = set()
    normalized: list[int] = []
    for value in root_ids:
        try:
            root_id = int(value)
        except (TypeError, ValueError):
            logger.debug("Skipping non-integer root id value: %r", value)
            continue
        if root_id not in seen:
            seen.add(root_id)
            normalized.append(root_id)
    return normalized


def _collect_annotations(
    n2_nodes: Sequence[int],
    n3_nodes: Sequence[int],
    *,
    annotation_fetcher: Optional[AnnotationFetcher],
    annotation_kwargs: Optional[Mapping[str, Any]],
    materialization: Optional[int],
) -> MutableMapping[int, Optional[Mapping[str, Any]]]:
    fetcher = annotation_fetcher or _default_annotation_fetcher
    nodes = sorted(set(int(node) for node in n2_nodes) | set(int(node) for node in n3_nodes))
    if not fetcher or not nodes:
        return {}

    kwargs: Dict[str, Any] = {}
    if annotation_kwargs:
        kwargs.update(annotation_kwargs)
    if materialization is not None and "materialization" not in kwargs:
        kwargs["materialization"] = materialization

    try:
        annotations = fetcher(nodes, **kwargs)
    except TypeError as exc:
        raise TypeError(
            "Annotation fetcher signature incompatible with provided keyword arguments"
        ) from exc

    if not isinstance(annotations, Mapping):
        raise TypeError("Annotation fetcher must return a mapping keyed by root id")

    normalized: Dict[int, Optional[Mapping[str, Any]]] = {}
    for key, value in annotations.items():
        try:
            root_id = int(key)
        except (TypeError, ValueError):  # pragma: no cover - defensive programming
            logger.debug("Discarding annotation entry with invalid key: %r", key)
            continue
        normalized[root_id] = value if value is None or isinstance(value, Mapping) else dict(value)
    return normalized


def _nodes_dataframe(
    root_ids: Sequence[int],
    annotations: Mapping[int, Optional[Mapping[str, Any]]],
) -> pd.DataFrame:
    if not root_ids:
        return pd.DataFrame({"pt_root_id": pd.Series(dtype="int64")})

    rows: list[Dict[str, Any]] = []
    additional_keys: set[str] = set()
    for root_id in sorted(int(node) for node in root_ids):
        row: Dict[str, Any] = {"pt_root_id": root_id}
        data = annotations.get(root_id)
        if isinstance(data, Mapping):
            for key, value in data.items():
                if key == "pt_root_id":
                    continue
                row[str(key)] = value
                additional_keys.add(str(key))
        rows.append(row)

    columns = ["pt_root_id"] + sorted(additional_keys)
    frame = pd.DataFrame(rows)
    return frame.reindex(columns=columns)


def _write_outputs(
    adjacency: pd.DataFrame,
    n2_nodes: pd.DataFrame,
    n3_nodes: pd.DataFrame,
    output_dir: Path,
) -> None:
    adjacency_path = output_dir / ADJACENCY_FILENAME
    n2_path = output_dir / N2_NODES_FILENAME
    n3_path = output_dir / N3_NODES_FILENAME

    try:
        adjacency.to_parquet(adjacency_path, index=False)
    except ImportError as exc:  # pragma: no cover - pandas raises this when pyarrow is missing
        raise ImportError(
            "Saving adjacency to Parquet requires pyarrow or fastparquet to be installed."
        ) from exc

    n2_nodes.to_csv(n2_path, index=False)
    n3_nodes.to_csv(n3_path, index=False)


def _empty_adjacency_frame() -> pd.DataFrame:
    return pd.DataFrame({
        "pre_pt_root_id": pd.Series(dtype="int64"),
        "post_pt_root_id": pd.Series(dtype="int64"),
        "synapse_count": pd.Series(dtype="int64"),
    })


__all__ = [
    "trace_next_layer",
    "TraceResult",
    "DEFAULT_MIN_SYNAPSES",
    "DEFAULT_OUTPUT_DIR",
    "DEFAULT_SYNAPSE_TABLE",
    "ADJACENCY_FILENAME",
    "N2_NODES_FILENAME",
    "N3_NODES_FILENAME",
]

