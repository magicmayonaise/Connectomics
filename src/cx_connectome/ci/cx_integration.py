"""Utilities for building CX signed effective-connectivity atlases.

This module provides tools for loading baseline stream annotations,
projecting signed effective-connectivity primitives onto those streams,
and persisting the resulting heatmap-friendly summaries.  It is designed
so it can be used programmatically in tests as well as from the
``ci_cli.py`` helper script.
"""

from __future__ import annotations

import csv
import json
import math
import os
from dataclasses import dataclass, field
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, Iterator, List, Mapping, Optional, Sequence, Tuple

try:  # Optional dependency for Parquet/Feather support.
    import pandas as _pd  # type: ignore
except Exception:  # pragma: no cover - pandas may be unavailable
    _pd = None  # type: ignore


DEFAULT_SCOPE = "cx"
DEFAULT_K_VALUES: Tuple[int, ...] = (1, 2, 3, 4, 5)
DEFAULT_SEED_CATEGORIES: Tuple[str, ...] = ("visual", "ascending", "central", "drive_state")

_CATEGORY_ALIASES: Mapping[str, str] = {
    "visual": "visual",
    "vision": "visual",
    "optic": "visual",
    "ascending": "ascending",
    "ascending_vnc": "ascending",
    "vnc": "ascending",
    "central": "central",
    "central_complex": "central",
    "cx": "central",
    "drive": "drive_state",
    "state": "drive_state",
    "drive_state": "drive_state",
    "drive/state": "drive_state",
}


@dataclass(frozen=True)
class StreamDefinition:
    """Describes a PFN/hΔ/PFL stream from the baseline annotations."""

    key: str
    label: str
    pfn_roots: Tuple[str, ...]
    h_delta_roots: Tuple[str, ...]
    pfl_roots: Tuple[str, ...]

    def all_members(self) -> Tuple[str, ...]:
        """Return every root-id associated with the stream."""

        return self.pfn_roots + self.h_delta_roots + self.pfl_roots


@dataclass
class BaselineAnnotations:
    """Container describing stream definitions and seed sets."""

    scope: str
    streams: Mapping[str, StreamDefinition]
    seed_sets: Mapping[str, Tuple[str, ...]]
    root_to_stream: Mapping[str, str] = field(default_factory=dict)
    seed_to_categories: Mapping[str, Tuple[str, ...]] = field(default_factory=dict)
    source_path: Optional[Path] = None

    def __post_init__(self) -> None:  # pragma: no cover - dataclass hook
        if not self.root_to_stream:
            object.__setattr__(
                self,
                "root_to_stream",
                {root: key for key, stream in self.streams.items() for root in stream.all_members()},
            )
        if not self.seed_to_categories:
            seed_to_categories: Dict[str, List[str]] = {}
            for category, roots in self.seed_sets.items():
                for root in roots:
                    seed_to_categories.setdefault(root, []).append(category)
            object.__setattr__(
                self,
                "seed_to_categories",
                {root: tuple(categories) for root, categories in seed_to_categories.items()},
            )


@dataclass
class AtlasResult:
    """Represents the signed atlas artefacts generated by this module."""

    scope: str
    categories: Tuple[str, ...]
    k_values: Tuple[int, ...]
    streams: Mapping[str, StreamDefinition]
    by_stream: Mapping[str, List[Mapping[str, Any]]]
    long_form: List[Mapping[str, Any]]
    metadata: Mapping[str, Any]
    output_paths: Mapping[str, Any]


def build_cx_signed_effective_connectivity_atlas(
    scope: str = DEFAULT_SCOPE,
    out_dir: Path | str = Path("out/ci/cx_atlas"),
    *,
    primitives: Optional[Path | str] = None,
    baseline: Optional[Path | str] = None,
    k_values: Sequence[int] = DEFAULT_K_VALUES,
    categories: Sequence[str] | None = None,
) -> AtlasResult:
    """Create the CX signed effective-connectivity atlas and persist the artefacts.

    Parameters
    ----------
    scope:
        Dataset scope used to resolve input/output locations.
    out_dir:
        Directory where artefacts should be written.
    primitives:
        Optional path to the signed effective-connectivity primitives.  May
        refer to a directory or a concrete file.
    baseline:
        Optional path to the baseline annotations JSON file.  If omitted the
        function looks for package data bundled with the module.
    k_values:
        Ordered sequence of path-length (k) values to include in the atlas.
    categories:
        Optional iterable overriding the default seed category order.
    """

    k_values = _normalise_k_values(k_values)
    category_order = tuple(categories) if categories else DEFAULT_SEED_CATEGORIES

    annotations = load_baseline_annotations(scope=scope, annotations_path=baseline)
    primitive_records, primitive_source = load_signed_effective_connectivity_primitives(scope, primitives)

    by_stream, long_rows = compute_signed_inflow_heatmaps(
        primitive_records,
        annotations,
        k_values=k_values,
        categories=category_order,
    )

    out_path = Path(out_dir)
    output_paths = persist_atlas(
        scope=scope,
        annotations=annotations,
        by_stream=by_stream,
        long_rows=long_rows,
        categories=category_order,
        k_values=k_values,
        out_dir=out_path,
        primitive_source=primitive_source,
    )

    metadata = {
        "scope": scope,
        "k_values": list(k_values),
        "categories": list(category_order),
        "stream_count": len(annotations.streams),
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "baseline_annotations_path": str(annotations.source_path) if annotations.source_path else None,
        "primitive_source": str(primitive_source) if primitive_source else None,
    }

    return AtlasResult(
        scope=scope,
        categories=category_order,
        k_values=k_values,
        streams=annotations.streams,
        by_stream=by_stream,
        long_form=long_rows,
        metadata=metadata,
        output_paths=output_paths,
    )


def load_baseline_annotations(
    *, scope: str = DEFAULT_SCOPE, annotations_path: Optional[Path | str] = None
) -> BaselineAnnotations:
    """Load baseline stream definitions and seed sets for the given scope."""

    data, source_path = _read_baseline_payload(scope, annotations_path)

    streams: Dict[str, StreamDefinition] = {}
    for raw_stream in data.get("streams", []):
        stream = _normalise_stream_record(raw_stream)
        streams[stream.key] = stream

    if not streams:
        raise ValueError("No stream definitions found in baseline annotations")

    seed_sets = _normalise_seed_sets(data.get("seed_sets", {}))
    if not seed_sets:
        raise ValueError("No seed sets defined in baseline annotations")

    annotations = BaselineAnnotations(
        scope=scope,
        streams=streams,
        seed_sets=seed_sets,
        source_path=source_path,
    )
    return annotations


def load_signed_effective_connectivity_primitives(
    scope: str = DEFAULT_SCOPE, primitives: Optional[Path | str] = None
) -> Tuple[List[Mapping[str, Any]], Optional[Path]]:
    """Load signed effective-connectivity primitives for the provided scope."""

    primitive_path = _resolve_primitive_path(scope, primitives)
    if primitive_path is None:
        raise FileNotFoundError(
            "Unable to locate signed effective-connectivity primitives. "
            "Provide the --primitives option or place the data under out/ci/primitives/."
        )

    records = list(_load_records_from_path(primitive_path))
    if not records:
        raise ValueError(f"No records loaded from primitives at {primitive_path}")
    return records, primitive_path


def compute_signed_inflow_heatmaps(
    records: Iterable[Mapping[str, Any]],
    annotations: BaselineAnnotations,
    *,
    k_values: Sequence[int],
    categories: Sequence[str],
) -> Tuple[Dict[str, List[Dict[str, Any]]], List[Dict[str, Any]]]:
    """Aggregate signed inflow by stream, seed category, and k."""

    k_values = _normalise_k_values(k_values)
    category_order = tuple(categories)

    per_stream: Dict[str, Dict[int, Dict[str, float]]] = {
        stream_key: {k: {category: 0.0 for category in category_order} for k in k_values}
        for stream_key in annotations.streams
    }

    for record in records:
        k = _extract_k(record)
        if k not in k_values:
            continue

        stream_key = _extract_stream_key(record, annotations)
        if stream_key is None:
            continue

        weight = _extract_signed_weight(record)
        if weight is None or (isinstance(weight, (int, float)) and math.isclose(weight, 0.0)):
            continue

        record_categories = _extract_categories(record, annotations, category_order)
        if not record_categories:
            continue

        for category in record_categories:
            per_stream[stream_key][k][category] += float(weight)

    by_stream: Dict[str, List[Dict[str, Any]]] = {}
    long_rows: List[Dict[str, Any]] = []

    for stream_key, stream in annotations.streams.items():
        stream_rows: List[Dict[str, Any]] = []
        for k in k_values:
            row = {"stream": stream_key, "stream_label": stream.label, "k": k}
            for category in category_order:
                row[category] = per_stream[stream_key][k][category]
                long_rows.append(
                    {
                        "stream": stream_key,
                        "stream_label": stream.label,
                        "k": k,
                        "category": category,
                        "value": per_stream[stream_key][k][category],
                    }
                )
            stream_rows.append(row)
        by_stream[stream_key] = stream_rows

    return by_stream, long_rows


def persist_atlas(
    *,
    scope: str,
    annotations: BaselineAnnotations,
    by_stream: Mapping[str, List[Mapping[str, Any]]],
    long_rows: Sequence[Mapping[str, Any]],
    categories: Sequence[str],
    k_values: Sequence[int],
    out_dir: Path,
    primitive_source: Optional[Path] = None,
) -> Dict[str, Any]:
    """Write atlas artefacts (CSV + JSON summaries) to disk."""

    out_dir.mkdir(parents=True, exist_ok=True)
    heatmap_dir = out_dir / "heatmaps"
    heatmap_dir.mkdir(exist_ok=True)

    # Write per-stream wide-format CSV tables.
    for stream_key, rows in by_stream.items():
        csv_path = heatmap_dir / f"{stream_key}.csv"
        _write_csv(csv_path, rows, fieldnames=["stream", "stream_label", "k", *categories])

    long_csv_path = out_dir / f"{scope}_signed_effective_connectivity_atlas_long.csv"
    _write_csv(long_csv_path, long_rows, fieldnames=["stream", "stream_label", "k", "category", "value"])

    atlas_payload = {
        "scope": scope,
        "k_values": list(k_values),
        "categories": list(categories),
        "primitive_source": str(primitive_source) if primitive_source else None,
        "baseline_annotations_path": str(annotations.source_path) if annotations.source_path else None,
        "streams": {
            stream_key: {
                "label": stream.label,
                "pfn_roots": list(stream.pfn_roots),
                "h_delta_roots": list(stream.h_delta_roots),
                "pfl_roots": list(stream.pfl_roots),
                "heatmap_csv": str((heatmap_dir / f"{stream_key}.csv").resolve()),
            }
            for stream_key, stream in annotations.streams.items()
        },
    }
    atlas_json_path = out_dir / f"{scope}_signed_effective_connectivity_atlas.json"
    _write_json(atlas_json_path, atlas_payload)

    metadata_path = out_dir / f"{scope}_signed_effective_connectivity_atlas_metadata.json"
    _write_json(
        metadata_path,
        {
            "scope": scope,
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "stream_count": len(annotations.streams),
            "k_values": list(k_values),
            "categories": list(categories),
            "primitive_source": str(primitive_source) if primitive_source else None,
            "baseline_annotations_path": str(annotations.source_path) if annotations.source_path else None,
        },
    )

    return {
        "base_dir": out_dir,
        "heatmaps": heatmap_dir,
        "long_csv": long_csv_path,
        "atlas_json": atlas_json_path,
        "metadata_json": metadata_path,
    }


# ---------------------------------------------------------------------------
# Baseline annotation helpers
# ---------------------------------------------------------------------------

def _read_baseline_payload(scope: str, annotations_path: Optional[Path | str]) -> Tuple[Mapping[str, Any], Optional[Path]]:
    if annotations_path is not None:
        candidate = Path(annotations_path)
        if not candidate.exists():
            raise FileNotFoundError(f"Baseline annotations not found at {candidate}")
        with candidate.open("r", encoding="utf-8") as handle:
            return json.load(handle), candidate

    # Search environment override first.
    env_dir = os.environ.get("CX_BASELINE_DIR")
    if env_dir:
        candidate = Path(env_dir) / f"{scope}_baseline_annotations.json"
        if candidate.exists():
            with candidate.open("r", encoding="utf-8") as handle:
                return json.load(handle), candidate

    # Fall back to package resources bundled with the project.
    try:  # Python 3.9+: importlib.resources.files
        import importlib.resources as resources

        package_name = "cx_connectome.ci.data"
        resource_name = f"{scope}_baseline_annotations.json"
        with resources.open_text(package_name, resource_name, encoding="utf-8") as handle:
            return json.load(handle), None
    except FileNotFoundError as exc:
        raise FileNotFoundError(
            "Unable to locate baseline annotations. Provide the --baseline option "
            "or set CX_BASELINE_DIR."
        ) from exc


def _normalise_stream_record(raw_stream: Mapping[str, Any]) -> StreamDefinition:
    if "stream_key" in raw_stream:
        key = str(raw_stream["stream_key"]).strip()
    elif "key" in raw_stream:
        key = str(raw_stream["key"]).strip()
    else:
        raise ValueError("Stream record missing required 'stream_key' field")

    label = str(raw_stream.get("display_name") or raw_stream.get("label") or key)

    def _normalise_member_list(value: Any) -> Tuple[str, ...]:
        members: List[str] = []
        if isinstance(value, (list, tuple, set)):
            iterable = value
        elif value is None:
            iterable = []
        else:
            iterable = [value]
        for item in iterable:
            normalised = _normalise_root_id(item)
            if normalised is not None:
                members.append(normalised)
        return tuple(members)

    pfn_roots = _normalise_member_list(
        raw_stream.get("pfn_roots") or raw_stream.get("pfn") or raw_stream.get("pfn_members")
    )
    h_delta_roots = _normalise_member_list(
        raw_stream.get("h_delta_roots")
        or raw_stream.get("hDelta_roots")
        or raw_stream.get("hΔ_roots")
        or raw_stream.get("h_delta")
        or raw_stream.get("hDelta")
        or raw_stream.get("hΔ")
    )
    pfl_roots = _normalise_member_list(
        raw_stream.get("pfl_roots") or raw_stream.get("pfl") or raw_stream.get("pfl_members")
    )

    return StreamDefinition(
        key=key,
        label=label,
        pfn_roots=pfn_roots,
        h_delta_roots=h_delta_roots,
        pfl_roots=pfl_roots,
    )


def _normalise_seed_sets(raw_seed_sets: Mapping[str, Any]) -> Mapping[str, Tuple[str, ...]]:
    seed_sets: Dict[str, Tuple[str, ...]] = {}
    for raw_key, raw_value in raw_seed_sets.items():
        key = _normalise_category_key(raw_key)
        if key is None:
            continue
        members: List[str] = []
        if isinstance(raw_value, Mapping):
            raw_iterable = raw_value.get("members") or raw_value.get("roots") or raw_value.values()
            if isinstance(raw_iterable, Mapping):
                raw_iterable = raw_iterable.values()
        else:
            raw_iterable = raw_value

        if isinstance(raw_iterable, (list, tuple, set)):
            iterable = raw_iterable
        else:
            iterable = [raw_iterable]

        for item in iterable:
            normalised = _normalise_root_id(item)
            if normalised is not None:
                members.append(normalised)
        seed_sets[key] = tuple(sorted(set(members)))
    return seed_sets


# ---------------------------------------------------------------------------
# Primitive parsing helpers
# ---------------------------------------------------------------------------

def _resolve_primitive_path(scope: str, primitives: Optional[Path | str]) -> Optional[Path]:
    if primitives is not None:
        candidate = Path(primitives)
        if candidate.is_file():
            return candidate
        if candidate.is_dir():
            located = _find_primitive_file(candidate)
            if located:
                return located
            raise FileNotFoundError(f"No primitive file found under {candidate}")

    # Search default output structure.
    default_dirs = [
        Path("out/ci/primitives") / scope,
        Path("out/ci") / scope / "primitives",
        Path("out/ci/primitives"),
    ]
    for directory in default_dirs:
        if directory.is_file():
            return directory
        if directory.is_dir():
            located = _find_primitive_file(directory)
            if located:
                return located
    return None


def _find_primitive_file(directory: Path) -> Optional[Path]:
    patterns = [
        "signed_effective_connectivity",
        "effective_connectivity_signed",
        "signed_effective_inflow",
        "signed_inflow",
    ]
    preferred_extensions = [".parquet", ".feather", ".csv", ".tsv", ".json"]

    candidates: List[Path] = []
    for path in sorted(directory.rglob("*")):
        if not path.is_file():
            continue
        name = path.name.lower()
        if any(pattern in name for pattern in patterns) and path.suffix.lower() in preferred_extensions:
            candidates.append(path)
    if not candidates:
        return None

    # Prefer files nearer the root directory by sorting on parts length.
    candidates.sort(key=lambda p: (len(p.parts), preferred_extensions.index(p.suffix.lower())))
    return candidates[0]


def _load_records_from_path(path: Path) -> Iterator[Mapping[str, Any]]:
    suffix = path.suffix.lower()
    if suffix == ".json":
        with path.open("r", encoding="utf-8") as handle:
            payload = json.load(handle)
        if isinstance(payload, Mapping):
            if "data" in payload and isinstance(payload["data"], list):
                items = payload["data"]
            else:
                items = [payload]
        elif isinstance(payload, list):
            items = payload
        else:
            raise ValueError(f"Unsupported JSON payload in {path}")
        for item in items:
            if isinstance(item, Mapping):
                yield item
    elif suffix in {".csv", ".tsv"}:
        delimiter = "," if suffix == ".csv" else "\t"
        with path.open("r", encoding="utf-8", newline="") as handle:
            reader = csv.DictReader(handle, delimiter=delimiter)
            for row in reader:
                yield row
    elif suffix in {".parquet", ".feather"}:
        if _pd is None:
            raise ImportError(
                f"Reading {suffix} files requires pandas; please install pandas to load {path}"
            )
        if suffix == ".parquet":
            frame = _pd.read_parquet(path)  # type: ignore[attr-defined]
        else:
            frame = _pd.read_feather(path)  # type: ignore[attr-defined]
        for record in frame.to_dict(orient="records"):
            yield record
    else:  # pragma: no cover - future formats can be added as needed
        raise ValueError(f"Unsupported primitive file format: {path.suffix}")


# ---------------------------------------------------------------------------
# Record parsing utilities
# ---------------------------------------------------------------------------

def _extract_k(record: Mapping[str, Any]) -> int:
    for key in ("k", "order", "path_length", "hop"):
        if key in record and record[key] not in (None, ""):
            try:
                return int(record[key])
            except (TypeError, ValueError):
                continue
    return -1


def _extract_stream_key(record: Mapping[str, Any], annotations: BaselineAnnotations) -> Optional[str]:
    explicit_keys = (
        "stream",
        "stream_key",
        "target_stream",
        "target_stream_key",
    )
    for key in explicit_keys:
        if key in record and record[key]:
            candidate = str(record[key]).strip()
            if candidate in annotations.streams:
                return candidate

    for key in ("target_root", "target_root_id", "target", "target_id"):
        if key in record and record[key] not in (None, ""):
            candidate = _normalise_root_id(record[key])
            if candidate is not None and candidate in annotations.root_to_stream:
                return annotations.root_to_stream[candidate]
    return None


def _extract_categories(
    record: Mapping[str, Any],
    annotations: BaselineAnnotations,
    category_order: Sequence[str],
) -> Tuple[str, ...]:
    categories: List[str] = []
    raw_values: List[Any] = []
    for key in (
        "source_category",
        "source_categories",
        "seed_set",
        "seed_sets",
        "source_seed_set",
        "source_seed_sets",
    ):
        if key in record and record[key] not in (None, ""):
            raw_values.append(record[key])

    for value in raw_values:
        categories.extend(_normalise_categories(value))

    if not categories:
        for key in ("source_root", "source_root_id", "seed", "seed_id"):
            if key in record and record[key] not in (None, ""):
                candidate = _normalise_root_id(record[key])
                if candidate is not None and candidate in annotations.seed_to_categories:
                    categories.extend(annotations.seed_to_categories[candidate])

    normalised = []
    seen = set()
    for category in categories:
        canonical = _normalise_category_key(category)
        if canonical and canonical not in seen and canonical in category_order:
            normalised.append(canonical)
            seen.add(canonical)
    return tuple(normalised)


def _extract_signed_weight(record: Mapping[str, Any]) -> Optional[float]:
    numeric_keys = (
        "signed_weight",
        "signed_flow",
        "signed_value",
        "weight",
        "flow",
        "value",
        "strength",
        "magnitude",
    )
    weight: Optional[float] = None
    for key in numeric_keys:
        if key in record and record[key] not in (None, ""):
            try:
                weight = float(record[key])
                break
            except (TypeError, ValueError):
                continue

    if weight is None:
        return None

    sign = record.get("sign") or record.get("direction")
    if sign is not None:
        if isinstance(sign, (int, float)):
            if sign < 0 and weight > 0:
                weight *= -1.0
        elif isinstance(sign, str):
            lowered = sign.strip().lower()
            if lowered in {"-", "neg", "negative", "inhibitory"} and weight > 0:
                weight *= -1.0
            elif lowered in {"+", "pos", "positive", "excitatory"}:
                weight = abs(weight)
    return weight


# ---------------------------------------------------------------------------
# Utility helpers
# ---------------------------------------------------------------------------

def _normalise_k_values(values: Sequence[int]) -> Tuple[int, ...]:
    normalised = []
    for value in values:
        try:
            integer = int(value)
        except (TypeError, ValueError):
            continue
        if integer > 0 and integer not in normalised:
            normalised.append(integer)
    if not normalised:
        raise ValueError("At least one positive k value must be supplied")
    normalised.sort()
    return tuple(normalised)


def _normalise_root_id(value: Any) -> Optional[str]:
    if value is None or value == "":
        return None
    if isinstance(value, str):
        stripped = value.strip()
        if not stripped:
            return None
        try:
            return str(int(stripped))
        except ValueError:
            return stripped
    try:
        return str(int(value))
    except (TypeError, ValueError):
        return None


def _normalise_category_key(value: Any) -> Optional[str]:
    if value is None:
        return None
    key = str(value).strip().lower()
    return _CATEGORY_ALIASES.get(key)


def _normalise_categories(value: Any) -> List[str]:
    if isinstance(value, (list, tuple, set)):
        items = value
    elif isinstance(value, str):
        items = [part.strip() for part in value.replace("|", ",").split(",") if part.strip()]
    else:
        items = [value]
    categories: List[str] = []
    for item in items:
        canonical = _normalise_category_key(item)
        if canonical:
            categories.append(canonical)
    return categories


def _write_csv(path: Path, rows: Sequence[Mapping[str, Any]], *, fieldnames: Sequence[str]) -> None:
    with path.open("w", encoding="utf-8", newline="") as handle:
        writer = csv.DictWriter(handle, fieldnames=fieldnames)
        writer.writeheader()
        for row in rows:
            writer.writerow(row)


def _write_json(path: Path, payload: Mapping[str, Any]) -> None:
    with path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, indent=2, sort_keys=True)
        handle.write("\n")


__all__ = [
    "AtlasResult",
    "BaselineAnnotations",
    "StreamDefinition",
    "build_cx_signed_effective_connectivity_atlas",
    "compute_signed_inflow_heatmaps",
    "load_baseline_annotations",
    "load_signed_effective_connectivity_primitives",
]
